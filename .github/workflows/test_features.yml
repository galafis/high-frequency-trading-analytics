name: Feature Engineering Pipeline Tests
# Trigger the workflow on push, pull request, or manual dispatch
on:
  push:
    branches: [ main, develop ]
    paths:
      - 'features.py'
      - 'tests/test_features.py'
      - 'requirements.txt'
      - '.github/workflows/test_features.yml'
      - 'validate_data.py'
      - 'data/**'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'features.py'
      - 'tests/test_features.py'
      - 'requirements.txt'
      - '.github/workflows/test_features.yml'
      - 'validate_data.py'
      - 'data/**'
  workflow_dispatch:  # Allow manual triggering

# Define environment variables
env:
  PYTHON_VERSION: '3.9'
  COVERAGE_THRESHOLD: 80

jobs:
  # Data validation job - runs before all other tests
  validate-data-schema:
    name: Validate Data Schema
    runs-on: ubuntu-latest
    
    steps:
    # Checkout the repository
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    # Set up Python environment
    - name: Set up Python 3.9
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        cache: 'pip'
    
    # Install dependencies for data validation
    - name: Install validation dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas pathlib
    
    # Create test data if it doesn't exist
    - name: Create test data directory and sample file
      run: |
        mkdir -p data/raw
        # Create a sample CSV file with correct schema for testing
        cat > data/raw/exemplo.csv << 'EOF'
        timestamp,price
        2023-01-01 09:00:00,100.50
        2023-01-01 09:00:01,100.52
        2023-01-01 09:00:02,100.48
        2023-01-01 09:00:03,100.55
        2023-01-01 09:00:04,100.53
        EOF
    
    # Run data schema validation
    - name: Run data schema validation
      run: |
        echo "ðŸ” Validating data schema before running tests..."
        python validate_data.py data/raw/exemplo.csv
        echo "âœ… Data schema validation passed!"
    
    # Test with intentionally invalid data to ensure validation works
    - name: Test schema validation with invalid data (negative test)
      run: |
        echo "ðŸ§ª Testing schema validation with invalid data..."
        # Create invalid data file (missing required column)
        cat > data/raw/invalid_exemplo.csv << 'EOF'
        datetime,value
        2023-01-01 09:00:00,100.50
        EOF
        
        # This should fail - if it doesn't, the validation isn't working properly
        if python validate_data.py data/raw/invalid_exemplo.csv 2>/dev/null; then
          echo "âŒ ERROR: Validation should have failed for invalid data!"
          exit 1
        else
          echo "âœ… Schema validation correctly rejected invalid data"
        fi
        
        # Clean up invalid test file
        rm data/raw/invalid_exemplo.csv

  test-features:
    name: Test Feature Engineering Pipeline
    runs-on: ubuntu-latest
    needs: validate-data-schema  # Only run if data validation passes
    
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
      fail-fast: false  # Continue testing other versions if one fails
    
    steps:
    # Checkout the repository
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full git history for better coverage reports
    
    # Set up Python environment
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'  # Cache pip dependencies
    
    # Upgrade pip and install build tools
    - name: Upgrade pip and install build tools
      run: |
        python -m pip install --upgrade pip
        pip install wheel setuptools
    
    # Install project dependencies
    - name: Install dependencies
      run: |
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        # Install testing and coverage dependencies
        pip install pytest pytest-cov pytest-html pytest-xdist
        # Install additional dependencies for robust testing
        pip install coverage pytest-mock pytest-benchmark
    
    # Create test data (replicating from validation job)
    - name: Ensure test data exists
      run: |
        mkdir -p data/raw
        if [ ! -f data/raw/exemplo.csv ]; then
          cat > data/raw/exemplo.csv << 'EOF'
        timestamp,price
        2023-01-01 09:00:00,100.50
        2023-01-01 09:00:01,100.52
        2023-01-01 09:00:02,100.48
        2023-01-01 09:00:03,100.55
        2023-01-01 09:00:04,100.53
        EOF
        fi
    
    # Re-run data validation to ensure consistency
    - name: Validate data schema before tests
      run: |
        echo "ðŸ” Re-validating data schema before feature tests..."
        python validate_data.py data/raw/exemplo.csv
    
    # Run linting and code quality checks
    - name: Lint with flake8
      run: |
        pip install flake8
        # Stop the build if there are Python syntax errors or undefined names
        flake8 features.py tests/test_features.py validate_data.py --count --select=E9,F63,F7,F82 --show-source --statistics
        # Treat all other issues as warnings
        flake8 features.py tests/test_features.py validate_data.py --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
    
    # Run the feature engineering tests with coverage
    - name: Run pytest with coverage
      run: |
        # Run tests with coverage reporting
        python -m pytest tests/test_features.py -v \
          --cov=features \
          --cov-report=html:htmlcov \
          --cov-report=xml:coverage.xml \
          --cov-report=term \
          --cov-fail-under=${{ env.COVERAGE_THRESHOLD }} \
          --html=pytest_report.html \
          --self-contained-html \
          --junit-xml=pytest_junit.xml
    
    # Upload coverage to codecov (optional)
    - name: Upload coverage to Codecov
      if: matrix.python-version == '3.9'  # Only upload once
      uses: codecov/codecov-action@v4
      with:
        files: ./coverage.xml
        flags: feature-engineering
        name: feature-pipeline-coverage
        fail_ci_if_error: false  # Don't fail if codecov is down
        token: ${{ secrets.CODECOV_TOKEN }}  # Required for v4
    
    # Upload test artifacts - UPDATED to v4
    - name: Upload test artifacts
      if: always()  # Upload even if tests fail
      uses: actions/upload-artifact@v4
      with:
        name: test-results-python-${{ matrix.python-version }}
        path: |
          htmlcov/
          pytest_report.html
          pytest_junit.xml
          coverage.xml
        retention-days: 30
    
    # Performance benchmark tests
    - name: Run performance benchmarks
      if: matrix.python-version == '3.9'  # Only run benchmarks on one version
      run: |
        # Run performance tests if they exist
        if [ -f "tests/test_features_performance.py" ]; then
          python -m pytest tests/test_features_performance.py -v --benchmark-only
        else
          echo "No performance tests found, skipping..."
        fi
  
  # Additional job for testing with minimal dependencies
  test-minimal-deps:
    name: Test with minimal dependencies
    runs-on: ubuntu-latest
    needs: validate-data-schema  # Only run if data validation passes
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
    
    - name: Install minimal dependencies
      run: |
        pip install pandas numpy pytest
        # Only install absolutely essential packages
    
    - name: Create test data
      run: |
        mkdir -p data/raw
        cat > data/raw/exemplo.csv << 'EOF'
        timestamp,price
        2023-01-01 09:00:00,100.50
        2023-01-01 09:00:01,100.52
        2023-01-01 09:00:02,100.48
        EOF
    
    - name: Validate data before minimal tests
      run: |
        python validate_data.py data/raw/exemplo.csv
    
    - name: Run basic feature tests
      run: |
        python -m pytest tests/test_features.py::TestFeatureEngineering::test_basic_features_creation -v
        python -m pytest tests/test_features.py::TestFeatureEngineering::test_minimal_data_requirements -v
  
  # Job to validate the features.py module can be imported
  validate-imports:
    name: Validate module imports
    runs-on: ubuntu-latest
    needs: validate-data-schema  # Only run if data validation passes
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install pandas numpy
    
    - name: Validate imports
      run: |
        python -c "from features import feature_engineering, export_features; print('âœ“ All imports successful')"
        python -c "import pandas as pd; import numpy as np; print('âœ“ Core dependencies available')"
        python -c "import validate_data; print('âœ“ Data validation module available')"
  
  # Summary job that depends on all test jobs
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [validate-data-schema, test-features, test-minimal-deps, validate-imports]
    if: always()  # Run even if some tests fail
    
    steps:
    - name: Test Results Summary
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.validate-data-schema.result }}" == "success" ]; then
          echo "âœ… Data schema validation: **PASSED**" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Data schema validation: **FAILED**" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.test-features.result }}" == "success" ]; then
          echo "âœ… Feature engineering tests: **PASSED**" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Feature engineering tests: **FAILED**" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.test-minimal-deps.result }}" == "success" ]; then
          echo "âœ… Minimal dependencies tests: **PASSED**" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Minimal dependencies tests: **FAILED**" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.validate-imports.result }}" == "success" ]; then
          echo "âœ… Import validation: **PASSED**" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Import validation: **FAILED**" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ“Š **Coverage Threshold:** ${{ env.COVERAGE_THRESHOLD }}%" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ”’ **Data Validation:** Schema validation enforced before all tests" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ”— **Workflow URL:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
    
    - name: Check overall success
      if: needs.validate-data-schema.result != 'success' || needs.test-features.result != 'success'
      run: |
        echo "Critical tests failed - data validation or feature tests failed"
        exit 1
